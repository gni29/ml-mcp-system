#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MCP Automatic Data Type Judgment & Pipeline Creation Demo
MCP ìë™ ë°ì´í„° íƒ€ì… íŒë‹¨ ë° íŒŒì´í”„ë¼ì¸ ìƒì„± ë°ëª¨
"""

import pandas as pd
import numpy as np
import json
from typing import Dict, Any, List

class AutoDataProcessor:
    """
    MCPì˜ ìë™ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ ë°ëª¨
    ì‹¤ì œ MCPì—ì„œ ì‚¬ìš©ë˜ëŠ” ìë™ íŒë‹¨ ë¡œì§
    """

    def __init__(self):
        self.column_analysis = {}
        self.preprocessing_pipeline = []
        self.analytics_pipeline = []

    def judge_data_types(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        1ë‹¨ê³„: ìë™ ë°ì´í„° íƒ€ì… íŒë‹¨
        Column A: numeric, Column B: categorical ë“± ìë™ ë¶„ë¥˜
        """
        print("ğŸ” Step 1: Automatic Data Type Judgment")
        print("=" * 50)

        type_analysis = {}

        for column in df.columns:
            col_data = df[column]

            # ê¸°ë³¸ íƒ€ì… ì •ë³´
            analysis = {
                'original_dtype': str(col_data.dtype),
                'non_null_count': col_data.count(),
                'null_count': col_data.isnull().sum(),
                'null_percentage': round(col_data.isnull().sum() / len(col_data) * 100, 2),
                'unique_count': col_data.nunique(),
                'unique_percentage': round(col_data.nunique() / len(col_data) * 100, 2)
            }

            # ìë™ íƒ€ì… ë¶„ë¥˜
            if pd.api.types.is_numeric_dtype(col_data):
                analysis['mcp_type'] = 'numeric'
                analysis['sub_type'] = self._classify_numeric(col_data)
                analysis.update({
                    'mean': col_data.mean(),
                    'median': col_data.median(),
                    'std': col_data.std(),
                    'min': col_data.min(),
                    'max': col_data.max(),
                    'has_outliers': self._detect_outliers(col_data),
                    'distribution': self._analyze_distribution(col_data)
                })

            elif pd.api.types.is_object_dtype(col_data):
                analysis['mcp_type'] = 'categorical'
                analysis['sub_type'] = self._classify_categorical(col_data)

                # ë¹ˆë„ ë¶„ì„
                value_counts = col_data.value_counts()
                analysis.update({
                    'most_frequent': value_counts.index[0] if len(value_counts) > 0 else None,
                    'most_frequent_count': value_counts.iloc[0] if len(value_counts) > 0 else 0,
                    'cardinality_level': self._classify_cardinality(col_data)
                })

                # ë‚ ì§œ/ì‹œê°„ ê°€ëŠ¥ì„± ì²´í¬
                if self._might_be_datetime(col_data):
                    analysis['potential_datetime'] = True

            elif pd.api.types.is_datetime64_any_dtype(col_data):
                analysis['mcp_type'] = 'datetime'
                analysis.update({
                    'date_range': f"{col_data.min()} to {col_data.max()}",
                    'time_span_days': (col_data.max() - col_data.min()).days
                })

            elif pd.api.types.is_bool_dtype(col_data):
                analysis['mcp_type'] = 'boolean'
                analysis.update({
                    'true_count': col_data.sum(),
                    'false_count': len(col_data) - col_data.sum()
                })

            type_analysis[column] = analysis

            # ì¶œë ¥
            print(f"ğŸ“Š {column}:")
            print(f"   Type: {analysis['mcp_type']} ({analysis.get('sub_type', 'standard')})")
            print(f"   Nulls: {analysis['null_percentage']:.1f}%")
            print(f"   Unique: {analysis['unique_count']} ({analysis['unique_percentage']:.1f}%)")

        self.column_analysis = type_analysis
        return type_analysis

    def create_preprocessing_pipeline(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        2ë‹¨ê³„: ìë™ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±
        ë°ì´í„° íƒ€ì…ì— ë”°ë¼ ë§ì¶¤í˜• ì „ì²˜ë¦¬ ë‹¨ê³„ ìƒì„±
        """
        print("\nğŸ”§ Step 2: Automatic Preprocessing Pipeline Creation")
        print("=" * 50)

        pipeline = []

        for column, analysis in self.column_analysis.items():
            mcp_type = analysis['mcp_type']

            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            if analysis['null_percentage'] > 0:
                if analysis['null_percentage'] > 50:
                    pipeline.append({
                        'step': 'remove_column',
                        'column': column,
                        'reason': f'High missing data: {analysis["null_percentage"]:.1f}%'
                    })
                    continue
                else:
                    # íƒ€ì…ë³„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•
                    if mcp_type == 'numeric':
                        method = 'median' if analysis.get('has_outliers') else 'mean'
                    elif mcp_type == 'categorical':
                        method = 'mode'
                    else:
                        method = 'forward_fill'

                    pipeline.append({
                        'step': 'handle_missing',
                        'column': column,
                        'method': method,
                        'reason': f'Fill {analysis["null_percentage"]:.1f}% missing values'
                    })

            # íƒ€ì…ë³„ ì „ì²˜ë¦¬
            if mcp_type == 'numeric':
                # ì´ìƒì¹˜ ì²˜ë¦¬
                if analysis.get('has_outliers'):
                    pipeline.append({
                        'step': 'handle_outliers',
                        'column': column,
                        'method': 'iqr_clipping',
                        'reason': 'Outliers detected'
                    })

                # ì •ê·œí™”/í‘œì¤€í™”
                if analysis['std'] > analysis['mean'] * 2:  # ë†’ì€ ë¶„ì‚°
                    pipeline.append({
                        'step': 'normalize',
                        'column': column,
                        'method': 'standard_scaling',
                        'reason': 'High variance detected'
                    })

            elif mcp_type == 'categorical':
                cardinality = analysis.get('cardinality_level')

                if cardinality == 'high':
                    pipeline.append({
                        'step': 'reduce_cardinality',
                        'column': column,
                        'method': 'top_categories',
                        'reason': f'High cardinality: {analysis["unique_count"]} categories'
                    })

                # ì¸ì½”ë”©
                if analysis['unique_count'] <= 10:
                    encoding_method = 'one_hot'
                else:
                    encoding_method = 'label_encoding'

                pipeline.append({
                    'step': 'encode_categorical',
                    'column': column,
                    'method': encoding_method,
                    'reason': f'Categorical encoding for {analysis["unique_count"]} categories'
                })

        # ì¤‘ë³µ ì œê±°
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            pipeline.append({
                'step': 'remove_duplicates',
                'reason': f'Found {duplicates} duplicate rows'
            })

        # ì¶œë ¥
        for step in pipeline:
            print(f"ğŸ”§ {step['step']}: {step.get('column', 'all')} - {step['reason']}")

        self.preprocessing_pipeline = pipeline
        return pipeline

    def create_analytics_pipeline(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        3ë‹¨ê³„: ìë™ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ìƒì„±
        ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ìµœì  ë¶„ì„ ë°©ë²• ì„ íƒ
        """
        print("\nğŸ“Š Step 3: Automatic Analytics Pipeline Creation")
        print("=" * 50)

        analytics = []

        # ë°ì´í„° ê°œìš”
        numeric_cols = [col for col, analysis in self.column_analysis.items()
                       if analysis['mcp_type'] == 'numeric']
        categorical_cols = [col for col, analysis in self.column_analysis.items()
                           if analysis['mcp_type'] == 'categorical']

        # 1. ê¸°ë³¸ í†µê³„ ë¶„ì„
        analytics.append({
            'analysis': 'descriptive_statistics',
            'scope': 'all_columns',
            'reason': 'Basic data understanding'
        })

        # 2. ìƒê´€ê´€ê³„ ë¶„ì„ (ìˆ«ìí˜• ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒ)
        if len(numeric_cols) >= 2:
            analytics.append({
                'analysis': 'correlation_analysis',
                'scope': numeric_cols,
                'reason': f'Multiple numeric variables ({len(numeric_cols)})'
            })

        # 3. ë¶„í¬ ë¶„ì„
        for col in numeric_cols:
            analysis = self.column_analysis[col]
            if analysis.get('has_outliers'):
                analytics.append({
                    'analysis': 'outlier_analysis',
                    'scope': [col],
                    'reason': f'Outliers detected in {col}'
                })

        # 4. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„
        for col in categorical_cols:
            analysis = self.column_analysis[col]
            if analysis['unique_count'] <= 20:  # ë²”ì£¼ê°€ ë„ˆë¬´ ë§ì§€ ì•Šìœ¼ë©´
                analytics.append({
                    'analysis': 'frequency_analysis',
                    'scope': [col],
                    'reason': f'Categorical distribution for {col}'
                })

        # 5. ê³ ê¸‰ ë¶„ì„ (ë°ì´í„° í¬ê¸°ì™€ íŠ¹ì„±ì— ë”°ë¼)
        if len(df) > 100 and len(numeric_cols) >= 3:
            analytics.append({
                'analysis': 'clustering_analysis',
                'scope': numeric_cols,
                'method': 'kmeans',
                'reason': 'Sufficient data for pattern detection'
            })

        if len(numeric_cols) >= 4:
            analytics.append({
                'analysis': 'dimensionality_reduction',
                'scope': numeric_cols,
                'method': 'pca',
                'reason': 'Multiple dimensions for PCA'
            })

        # 6. ì‹œê°í™”
        for col in numeric_cols:
            analytics.append({
                'analysis': 'visualization',
                'type': 'histogram',
                'scope': [col],
                'reason': f'Distribution visualization for {col}'
            })

        if len(numeric_cols) >= 2:
            analytics.append({
                'analysis': 'visualization',
                'type': 'correlation_heatmap',
                'scope': numeric_cols,
                'reason': 'Correlation visualization'
            })

        # ì¶œë ¥
        for step in analytics:
            scope_str = step['scope'] if isinstance(step['scope'], str) else f"{len(step['scope'])} columns"
            print(f"ğŸ“ˆ {step['analysis']}: {scope_str} - {step['reason']}")

        self.analytics_pipeline = analytics
        return analytics

    def _classify_numeric(self, series):
        """ìˆ«ìí˜• ì„¸ë¶€ ë¶„ë¥˜"""
        if series.dtype == 'int64' and series.nunique() <= 20:
            return 'ordinal'
        elif series.min() >= 0 and series.max() <= 1:
            return 'probability'
        elif series.dtype == 'int64':
            return 'integer'
        else:
            return 'continuous'

    def _classify_categorical(self, series):
        """ë²”ì£¼í˜• ì„¸ë¶€ ë¶„ë¥˜"""
        unique_count = series.nunique()
        if unique_count <= 5:
            return 'nominal_low'
        elif unique_count <= 20:
            return 'nominal_medium'
        else:
            return 'nominal_high'

    def _classify_cardinality(self, series):
        """ì¹´ë””ë„ë¦¬í‹° ë¶„ë¥˜"""
        unique_ratio = series.nunique() / len(series)
        if unique_ratio > 0.8:
            return 'high'
        elif unique_ratio > 0.3:
            return 'medium'
        else:
            return 'low'

    def _detect_outliers(self, series):
        """ì´ìƒì¹˜ ê°ì§€"""
        if series.dtype not in ['int64', 'float64']:
            return False
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return ((series < lower_bound) | (series > upper_bound)).any()

    def _analyze_distribution(self, series):
        """ë¶„í¬ ë¶„ì„"""
        skewness = series.skew()
        if abs(skewness) < 0.5:
            return 'normal'
        elif skewness > 0.5:
            return 'right_skewed'
        else:
            return 'left_skewed'

    def _might_be_datetime(self, series):
        """ë‚ ì§œ/ì‹œê°„ ê°€ëŠ¥ì„± ì²´í¬"""
        # ê°„ë‹¨í•œ ë‚ ì§œ íŒ¨í„´ ì²´í¬
        sample = series.dropna().astype(str).head(10)
        date_like = 0
        for value in sample:
            if any(pattern in value for pattern in ['-', '/', ':', 'T']):
                date_like += 1
        return date_like > len(sample) * 0.5

def demo_automatic_processing():
    """ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ"""
    print("ğŸš€ MCP Automatic Data Processing Demo")
    print("=" * 60)

    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    df = pd.DataFrame({
        'customer_id': range(1, 1001),  # ì •ìˆ˜í˜• ID
        'age': np.random.normal(35, 10, 1000).astype(int),  # ìˆ«ìí˜•
        'income': np.random.lognormal(10, 0.5, 1000),  # ìˆ«ìí˜• (ì¹˜ìš°ì¹œ ë¶„í¬)
        'category': np.random.choice(['A', 'B', 'C', 'D'], 1000),  # ë²”ì£¼í˜• (ë‚®ì€ ì¹´ë””ë„ë¦¬í‹°)
        'region': np.random.choice([f'Region_{i}' for i in range(50)], 1000),  # ë²”ì£¼í˜• (ë†’ì€ ì¹´ë””ë„ë¦¬í‹°)
        'is_premium': np.random.choice([True, False], 1000),  # ë¶ˆë¦°í˜•
        'signup_date': pd.date_range('2020-01-01', periods=1000, freq='D')  # ë‚ ì§œí˜•
    })

    # ì¼ë¶€ ê²°ì¸¡ì¹˜ ì¶”ê°€
    df.loc[np.random.choice(df.index, 50), 'income'] = np.nan
    df.loc[np.random.choice(df.index, 20), 'category'] = np.nan

    print(f"ğŸ“‹ Sample Data Shape: {df.shape}")
    print(f"ğŸ“‹ Columns: {list(df.columns)}")
    print()

    # ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰
    processor = AutoDataProcessor()

    # 1. ìë™ íƒ€ì… íŒë‹¨
    type_analysis = processor.judge_data_types(df)

    # 2. ìë™ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±
    preprocessing = processor.create_preprocessing_pipeline(df)

    # 3. ìë™ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ìƒì„±
    analytics = processor.create_analytics_pipeline(df)

    print("\nâœ… MCP Automatic Processing Complete!")
    print(f"ğŸ“Š Data Types Identified: {len(type_analysis)} columns")
    print(f"ğŸ”§ Preprocessing Steps: {len(preprocessing)} operations")
    print(f"ğŸ“ˆ Analytics Steps: {len(analytics)} analyses")

    return {
        'data_types': type_analysis,
        'preprocessing_pipeline': preprocessing,
        'analytics_pipeline': analytics
    }

if __name__ == "__main__":
    result = demo_automatic_processing()

    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥ (ì‹¤ì œ MCPì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•ì‹)
    with open('automatic_pipeline_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nğŸ’¾ Result saved to: automatic_pipeline_result.json")