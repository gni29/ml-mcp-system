/**
 * Machine Learning Service for ML MCP
 * ML MCPìš© ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤ - ê³ ê¸‰ ML ëª¨ë¸ë§ê³¼ ì˜ˆì¸¡ì— íŠ¹í™”
 */

import { spawn } from 'child_process';
import path from 'path';
import { fileURLToPath } from 'url';
import { BaseService } from 'ml-mcp-shared/utils/base-service';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

export class MachineLearningService extends BaseService {
  constructor(logger) {
    super('machine-learning-service', 'ml', '1.0.0');
    this.logger = logger;
    this.capabilities = ['tools'];
    this.modelCache = new Map(); // Cache for trained models
  }

  /**
   * Initialize the ML service
   */
  async initialize() {
    try {
      this.logger.info('ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘');

      // Test ML environment
      await this.testMLEnvironment();

      await super.initialize();
      this.logger.info('âœ… ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ');

    } catch (error) {
      this.logger.error('âŒ ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * Test ML environment
   */
  async testMLEnvironment() {
    return new Promise((resolve, reject) => {
      const pythonProcess = spawn('python', ['-c',
        'import sklearn, pandas, numpy, joblib; print("ML environment OK")'
      ]);

      pythonProcess.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error('ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (scikit-learn, pandas, numpy, joblib)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'));
        }
      });

      pythonProcess.on('error', (error) => {
        reject(new Error(`Python ML í™˜ê²½ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: ${error.message}`));
      });
    });
  }

  /**
   * Get available ML tools
   */
  async getTools() {
    return [
      {
        name: 'train_classifier',
        description: 'ë¶„ë¥˜ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤ (Random Forest, SVM, Logistic Regression ë“±)',
        inputSchema: {
          type: 'object',
          properties: {
            data_file: {
              type: 'string',
              description: 'í›ˆë ¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            target_column: {
              type: 'string',
              description: 'íƒ€ê²Ÿ(ë ˆì´ë¸”) ì»¬ëŸ¼ëª…'
            },
            model_type: {
              type: 'string',
              description: 'ëª¨ë¸ ìœ í˜•',
              enum: ['random_forest', 'svm', 'logistic_regression', 'gradient_boosting', 'neural_network'],
              default: 'random_forest'
            },
            test_size: {
              type: 'number',
              description: 'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨',
              default: 0.2,
              minimum: 0.1,
              maximum: 0.5
            },
            cross_validation: {
              type: 'boolean',
              description: 'êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì—¬ë¶€',
              default: true
            },
            save_model: {
              type: 'boolean',
              description: 'ëª¨ë¸ ì €ì¥ ì—¬ë¶€',
              default: true
            }
          },
          required: ['data_file', 'target_column']
        }
      },
      {
        name: 'train_regressor',
        description: 'íšŒê·€ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤ (Linear Regression, Random Forest, SVR ë“±)',
        inputSchema: {
          type: 'object',
          properties: {
            data_file: {
              type: 'string',
              description: 'í›ˆë ¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            target_column: {
              type: 'string',
              description: 'íƒ€ê²Ÿ(ì˜ˆì¸¡ ëŒ€ìƒ) ì»¬ëŸ¼ëª…'
            },
            model_type: {
              type: 'string',
              description: 'íšŒê·€ ëª¨ë¸ ìœ í˜•',
              enum: ['linear_regression', 'random_forest', 'svr', 'gradient_boosting', 'neural_network'],
              default: 'random_forest'
            },
            test_size: {
              type: 'number',
              description: 'í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨',
              default: 0.2,
              minimum: 0.1,
              maximum: 0.5
            },
            cross_validation: {
              type: 'boolean',
              description: 'êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì—¬ë¶€',
              default: true
            },
            save_model: {
              type: 'boolean',
              description: 'ëª¨ë¸ ì €ì¥ ì—¬ë¶€',
              default: true
            }
          },
          required: ['data_file', 'target_column']
        }
      },
      {
        name: 'hyperparameter_tuning',
        description: 'í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ìµœì ì˜ ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤',
        inputSchema: {
          type: 'object',
          properties: {
            data_file: {
              type: 'string',
              description: 'í›ˆë ¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            target_column: {
              type: 'string',
              description: 'íƒ€ê²Ÿ ì»¬ëŸ¼ëª…'
            },
            model_type: {
              type: 'string',
              description: 'íŠœë‹í•  ëª¨ë¸ ìœ í˜•',
              enum: ['random_forest', 'svm', 'gradient_boosting', 'neural_network'],
              default: 'random_forest'
            },
            task_type: {
              type: 'string',
              description: 'ì‘ì—… ìœ í˜•',
              enum: ['classification', 'regression'],
              default: 'classification'
            },
            search_method: {
              type: 'string',
              description: 'íƒìƒ‰ ë°©ë²•',
              enum: ['grid_search', 'random_search', 'bayesian_optimization'],
              default: 'grid_search'
            },
            cv_folds: {
              type: 'number',
              description: 'êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜',
              default: 5,
              minimum: 3,
              maximum: 10
            }
          },
          required: ['data_file', 'target_column', 'task_type']
        }
      },
      {
        name: 'feature_engineering',
        description: 'íŠ¹ì„± ê³µí•™ì„ í†µí•´ ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ í”¼ì²˜ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤',
        inputSchema: {
          type: 'object',
          properties: {
            data_file: {
              type: 'string',
              description: 'ì…ë ¥ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            operations: {
              type: 'array',
              description: 'ìˆ˜í–‰í•  íŠ¹ì„± ê³µí•™ ì‘ì—…ë“¤',
              items: {
                type: 'string',
                enum: ['scaling', 'normalization', 'encoding', 'polynomial_features', 'feature_selection', 'pca', 'interaction_features']
              },
              default: ['scaling', 'encoding']
            },
            target_column: {
              type: 'string',
              description: 'íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (íŠ¹ì„± ì„ íƒ ì‹œ í•„ìš”)'
            },
            output_file: {
              type: 'string',
              description: 'ë³€í™˜ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ',
              default: 'processed_data.csv'
            }
          },
          required: ['data_file']
        }
      },
      {
        name: 'model_evaluation',
        description: 'í›ˆë ¨ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤',
        inputSchema: {
          type: 'object',
          properties: {
            model_file: {
              type: 'string',
              description: 'í‰ê°€í•  ëª¨ë¸ íŒŒì¼ ê²½ë¡œ'
            },
            test_data_file: {
              type: 'string',
              description: 'í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            target_column: {
              type: 'string',
              description: 'íƒ€ê²Ÿ ì»¬ëŸ¼ëª…'
            },
            task_type: {
              type: 'string',
              description: 'ì‘ì—… ìœ í˜•',
              enum: ['classification', 'regression'],
              default: 'classification'
            },
            generate_plots: {
              type: 'boolean',
              description: 'ì‹œê°í™” í”Œë¡¯ ìƒì„± ì—¬ë¶€',
              default: true
            }
          },
          required: ['model_file', 'test_data_file', 'target_column', 'task_type']
        }
      },
      {
        name: 'make_predictions',
        description: 'í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤',
        inputSchema: {
          type: 'object',
          properties: {
            model_file: {
              type: 'string',
              description: 'ì˜ˆì¸¡ì— ì‚¬ìš©í•  ëª¨ë¸ íŒŒì¼ ê²½ë¡œ'
            },
            input_data_file: {
              type: 'string',
              description: 'ì˜ˆì¸¡í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            output_file: {
              type: 'string',
              description: 'ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ',
              default: 'predictions.csv'
            },
            include_probabilities: {
              type: 'boolean',
              description: 'ë¶„ë¥˜ í™•ë¥  í¬í•¨ ì—¬ë¶€ (ë¶„ë¥˜ ëª¨ë¸ì¸ ê²½ìš°)',
              default: false
            }
          },
          required: ['model_file', 'input_data_file']
        }
      },
      {
        name: 'clustering_analysis',
        description: 'ë¹„ì§€ë„ í•™ìŠµì„ í†µí•œ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤',
        inputSchema: {
          type: 'object',
          properties: {
            data_file: {
              type: 'string',
              description: 'í´ëŸ¬ìŠ¤í„°ë§í•  ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            algorithm: {
              type: 'string',
              description: 'í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜',
              enum: ['kmeans', 'hierarchical', 'dbscan', 'gaussian_mixture'],
              default: 'kmeans'
            },
            n_clusters: {
              type: 'number',
              description: 'í´ëŸ¬ìŠ¤í„° ìˆ˜ (K-means, Hierarchicalì¸ ê²½ìš°)',
              default: 3,
              minimum: 2,
              maximum: 20
            },
            auto_determine_clusters: {
              type: 'boolean',
              description: 'ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ìë™ ê²°ì •',
              default: true
            },
            include_visualization: {
              type: 'boolean',
              description: 'í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™” í¬í•¨',
              default: true
            }
          },
          required: ['data_file']
        }
      },
      {
        name: 'time_series_forecasting',
        description: 'ì‹œê³„ì—´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤',
        inputSchema: {
          type: 'object',
          properties: {
            data_file: {
              type: 'string',
              description: 'ì‹œê³„ì—´ ë°ì´í„° íŒŒì¼ ê²½ë¡œ'
            },
            date_column: {
              type: 'string',
              description: 'ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ëª…'
            },
            value_column: {
              type: 'string',
              description: 'ì˜ˆì¸¡í•  ê°’ì˜ ì»¬ëŸ¼ëª…'
            },
            forecast_periods: {
              type: 'number',
              description: 'ì˜ˆì¸¡í•  ê¸°ê°„ ìˆ˜',
              default: 30,
              minimum: 1,
              maximum: 365
            },
            model_type: {
              type: 'string',
              description: 'ì‹œê³„ì—´ ëª¨ë¸ ìœ í˜•',
              enum: ['arima', 'lstm', 'prophet', 'exponential_smoothing'],
              default: 'arima'
            },
            include_seasonality: {
              type: 'boolean',
              description: 'ê³„ì ˆì„± ê³ ë ¤ ì—¬ë¶€',
              default: true
            }
          },
          required: ['data_file', 'date_column', 'value_column']
        }
      }
    ];
  }

  /**
   * Execute ML tool
   */
  async executeTool(toolName, args) {
    if (!this.isInitialized) {
      throw new Error('ë¨¸ì‹ ëŸ¬ë‹ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    this.logger.info(`ML ë„êµ¬ ì‹¤í–‰ ì¤‘: ${toolName}`, args);

    switch (toolName) {
      case 'train_classifier':
        return await this.handleTrainClassifier(args);
      case 'train_regressor':
        return await this.handleTrainRegressor(args);
      case 'hyperparameter_tuning':
        return await this.handleHyperparameterTuning(args);
      case 'feature_engineering':
        return await this.handleFeatureEngineering(args);
      case 'model_evaluation':
        return await this.handleModelEvaluation(args);
      case 'make_predictions':
        return await this.handleMakePredictions(args);
      case 'clustering_analysis':
        return await this.handleClusteringAnalysis(args);
      case 'time_series_forecasting':
        return await this.handleTimeSeriesForecasting(args);
      default:
        throw new Error(`ì•Œ ìˆ˜ ì—†ëŠ” ML ë„êµ¬: ${toolName}`);
    }
  }

  /**
   * Handle classifier training
   */
  async handleTrainClassifier(args) {
    const { data_file, target_column, model_type = 'random_forest', test_size = 0.2, cross_validation = true, save_model = true } = args;

    try {
      const result = await this.runMLScript('train_classifier', {
        file_path: data_file,
        target_column,
        model_type,
        test_size,
        cross_validation,
        save_model
      });

      return {
        content: [{
          type: 'text',
          text: `**ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ**\n\n` +
                `**ëª¨ë¸ ìœ í˜•:** ${model_type}\n` +
                `**ë°ì´í„°:** ${data_file}\n` +
                `**íƒ€ê²Ÿ:** ${target_column}\n\n` +
                `**ì„±ëŠ¥ ì§€í‘œ:**\n` +
                `â€¢ ì •í™•ë„: ${result.accuracy || 'N/A'}\n` +
                `â€¢ ì •ë°€ë„: ${result.precision || 'N/A'}\n` +
                `â€¢ ì¬í˜„ìœ¨: ${result.recall || 'N/A'}\n` +
                `â€¢ F1 ì ìˆ˜: ${result.f1_score || 'N/A'}\n\n` +
                `${cross_validation ? `**êµì°¨ ê²€ì¦ ì ìˆ˜:** ${result.cv_scores?.join(', ') || 'N/A'}\n\n` : ''}` +
                `ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Handle regressor training
   */
  async handleTrainRegressor(args) {
    const { data_file, target_column, model_type = 'random_forest', test_size = 0.2, cross_validation = true, save_model = true } = args;

    try {
      const result = await this.runMLScript('train_regressor', {
        file_path: data_file,
        target_column,
        model_type,
        test_size,
        cross_validation,
        save_model
      });

      return {
        content: [{
          type: 'text',
          text: `**íšŒê·€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ**\n\n` +
                `**ëª¨ë¸ ìœ í˜•:** ${model_type}\n` +
                `**ë°ì´í„°:** ${data_file}\n` +
                `**íƒ€ê²Ÿ:** ${target_column}\n\n` +
                `**ì„±ëŠ¥ ì§€í‘œ:**\n` +
                `â€¢ RÂ² ì ìˆ˜: ${result.r2_score || 'N/A'}\n` +
                `â€¢ MAE: ${result.mae || 'N/A'}\n` +
                `â€¢ MSE: ${result.mse || 'N/A'}\n` +
                `â€¢ RMSE: ${result.rmse || 'N/A'}\n\n` +
                `${cross_validation ? `**êµì°¨ ê²€ì¦ ì ìˆ˜:** ${result.cv_scores?.join(', ') || 'N/A'}\n\n` : ''}` +
                `íšŒê·€ ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`íšŒê·€ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Handle hyperparameter tuning
   */
  async handleHyperparameterTuning(args) {
    const { data_file, target_column, model_type = 'random_forest', task_type = 'classification', search_method = 'grid_search', cv_folds = 5 } = args;

    try {
      const result = await this.runMLScript('hyperparameter_tuning', {
        file_path: data_file,
        target_column,
        model_type,
        task_type,
        search_method,
        cv_folds
      });

      return {
        content: [{
          type: 'text',
          text: `**í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ**\n\n` +
                `**ëª¨ë¸:** ${model_type}\n` +
                `**íƒìƒ‰ ë°©ë²•:** ${search_method}\n` +
                `**ì‘ì—… ìœ í˜•:** ${task_type}\n\n` +
                `**ìµœì  íŒŒë¼ë¯¸í„°:**\n` +
                `${Object.entries(result.best_params || {}).map(([k, v]) => `â€¢ ${k}: ${v}`).join('\n')}\n\n` +
                `**ìµœê³  ì„±ëŠ¥:** ${result.best_score || 'N/A'}\n\n` +
                `í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Handle feature engineering
   */
  async handleFeatureEngineering(args) {
    const { data_file, operations = ['scaling', 'encoding'], target_column, output_file = 'processed_data.csv' } = args;

    try {
      const result = await this.runMLScript('feature_engineering', {
        file_path: data_file,
        operations,
        target_column
      });

      return {
        content: [{
          type: 'text',
          text: `**íŠ¹ì„± ê³µí•™ ì™„ë£Œ**\\n\\n` +
                `**ì…ë ¥ ë°ì´í„°:** ${data_file}\\n` +
                `**ìˆ˜í–‰ ì‘ì—…:** ${operations.join(', ')}\\n` +
                `**ì¶œë ¥ íŒŒì¼:** ${output_file}\\n\\n` +
                `**ë³€í™˜ ê²°ê³¼:**\\n` +
                `â€¢ ì›ë³¸ íŠ¹ì„± ìˆ˜: ${result.original_features || 'N/A'}\\n` +
                `â€¢ ë³€í™˜ í›„ íŠ¹ì„± ìˆ˜: ${result.processed_features || 'N/A'}\\n` +
                `â€¢ ë°ì´í„° í¬ê¸°: ${result.data_shape || 'N/A'}\\n\\n` +
                `íŠ¹ì„± ê³µí•™ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`íŠ¹ì„± ê³µí•™ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Handle model evaluation
   */
  async handleModelEvaluation(args) {
    const { model_file, test_data_file, target_column, task_type = 'classification', generate_plots = true } = args;

    try {
      const result = await this.runMLScript('model_evaluation', {
        model_file,
        test_data_file,
        target_column,
        task_type,
        generate_plots
      });

      const metricsText = task_type === 'classification'
        ? `â€¢ ì •í™•ë„: ${result.accuracy || 'N/A'}\nâ€¢ ì •ë°€ë„: ${result.precision || 'N/A'}\nâ€¢ ì¬í˜„ìœ¨: ${result.recall || 'N/A'}\nâ€¢ F1 ì ìˆ˜: ${result.f1_score || 'N/A'}`
        : `â€¢ RÂ² ì ìˆ˜: ${result.r2_score || 'N/A'}\nâ€¢ MAE: ${result.mae || 'N/A'}\nâ€¢ MSE: ${result.mse || 'N/A'}\nâ€¢ RMSE: ${result.rmse || 'N/A'}`;

      return {
        content: [{
          type: 'text',
          text: `**ëª¨ë¸ í‰ê°€ ì™„ë£Œ**\n\n` +
                `**ëª¨ë¸ íŒŒì¼:** ${model_file}\n` +
                `**í…ŒìŠ¤íŠ¸ ë°ì´í„°:** ${test_data_file}\n` +
                `**ì‘ì—… ìœ í˜•:** ${task_type}\n\n` +
                `**ì„±ëŠ¥ ì§€í‘œ:**\n` +
                `${metricsText}\n\n` +
                `${generate_plots ? 'ì‹œê°í™” í”Œë¡¯ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n' : ''}` +
                `ëª¨ë¸ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Handle predictions
   */
  async handleMakePredictions(args) {
    const { model_file, input_data_file, output_file = 'predictions.csv', include_probabilities = false } = args;

    try {
      const result = await this.runMLScript('make_predictions', {
        model_file,
        input_data_file,
        output_file,
        include_probabilities
      });

      return {
        content: [{
          type: 'text',
          text: `**ì˜ˆì¸¡ ì™„ë£Œ**\n\n` +
                `**ëª¨ë¸:** ${model_file}\n` +
                `**ì…ë ¥ ë°ì´í„°:** ${input_data_file}\n` +
                `**ì¶œë ¥ íŒŒì¼:** ${output_file}\n\n` +
                `**ì˜ˆì¸¡ ê²°ê³¼:**\n` +
                `â€¢ ì˜ˆì¸¡ëœ ìƒ˜í”Œ ìˆ˜: ${result.predictions_count || 'N/A'}\n` +
                `â€¢ ê³ ìœ  ì˜ˆì¸¡ê°’ ìˆ˜: ${result.unique_predictions || 'N/A'}\n` +
                `${include_probabilities ? `â€¢ í™•ë¥  ì •ë³´ í¬í•¨: ì˜ˆ\n` : ''}\n` +
                `ì˜ˆì¸¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`ì˜ˆì¸¡ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Handle clustering analysis
   */
  async handleClusteringAnalysis(args) {
    const { data_file, algorithm = 'kmeans', n_clusters = 3, auto_determine_clusters = true, include_visualization = true } = args;

    try {
      const result = await this.runMLScript('clustering', {
        file_path: data_file,
        algorithm,
        n_clusters,
        auto_determine: auto_determine_clusters
      });

      return {
        content: [{
          type: 'text',
          text: `**í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ë£Œ**\\n\\n` +
                `**ì•Œê³ ë¦¬ì¦˜:** ${algorithm}\\n` +
                `**ë°ì´í„°:** ${data_file}\\n` +
                `**í´ëŸ¬ìŠ¤í„° ìˆ˜:** ${result.final_clusters || n_clusters}\\n\\n` +
                `**í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:**\\n` +
                `â€¢ ì‹¤ë£¨ì—£ ì ìˆ˜: ${result.silhouette_score || 'N/A'}\\n` +
                `â€¢ ê´€ì„±(Inertia): ${result.inertia || 'N/A'}\\n` +
                `â€¢ í´ëŸ¬ìŠ¤í„° í¬ê¸°: ${result.cluster_sizes?.join(', ') || 'N/A'}\\n\\n` +
                `${auto_determine_clusters ? `ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ìë™ìœ¼ë¡œ ê²°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\\n` : ''}` +
                `${include_visualization ? `í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\\n` : ''}\\n` +
                `í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Handle time series forecasting
   */
  async handleTimeSeriesForecasting(args) {
    const { data_file, date_column, value_column, forecast_periods = 30, model_type = 'arima', include_seasonality = true } = args;

    try {
      const result = await this.runMLScript('forecasting', {
        file_path: data_file,
        date_column,
        value_column,
        forecast_periods,
        model_type
      });

      return {
        content: [{
          type: 'text',
          text: `**ì‹œê³„ì—´ ì˜ˆì¸¡ ì™„ë£Œ**\\n\\n` +
                `**ëª¨ë¸:** ${model_type}\\n` +
                `**ë°ì´í„°:** ${data_file}\\n` +
                `**ì˜ˆì¸¡ ê¸°ê°„:** ${forecast_periods}\\n\\n` +
                `**ì˜ˆì¸¡ ì„±ëŠ¥:**\\n` +
                `â€¢ MAE: ${result.mae || 'N/A'}\\n` +
                `â€¢ MAPE: ${result.mape || 'N/A'}\\n` +
                `â€¢ RMSE: ${result.rmse || 'N/A'}\\n\\n` +
                `**ëª¨ë¸ ì •ë³´:**\\n` +
                `â€¢ í›ˆë ¨ ê¸°ê°„: ${result.train_period || 'N/A'}\\n` +
                `â€¢ ê³„ì ˆì„±: ${include_seasonality ? 'í¬í•¨' : 'ë¯¸í¬í•¨'}\\n\\n` +
                `ì‹œê³„ì—´ ì˜ˆì¸¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.`
        }]
      };
    } catch (error) {
      throw new Error(`ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹¤íŒ¨: ${error.message}`);
    }
  }

  /**
   * Get script path based on script name
   */
  getScriptPath(scriptName) {
    // ML MCP scripts are in python/ml/ directory
    const mlScripts = [
      'train_classifier', 'train_regressor', 'hyperparameter_tuning',
      'feature_engineering', 'model_evaluation', 'make_predictions',
      'clustering_analysis', 'time_series_forecasting'
    ];

    if (mlScripts.includes(scriptName)) {
      return path.join(__dirname, '..', 'python', 'ml', `${scriptName}.py`);
    }

    // Fallback to analyzers directory for other scripts
    const advancedScripts = ['clustering', 'outlier_detection', 'pca'];
    const timeseriesScripts = ['forecasting', 'seasonality', 'trend_analysis'];

    let subdir = 'basic';
    if (advancedScripts.includes(scriptName)) {
      subdir = 'advanced';
    } else if (timeseriesScripts.includes(scriptName)) {
      subdir = 'timeseries';
    }

    return path.join(__dirname, '..', '..', 'python', 'analyzers', subdir, `${scriptName}.py`);
  }

  /**
   * Run ML Python script
   */
  async runMLScript(scriptName, options = {}) {
    const scriptPath = this.getScriptPath(scriptName);

    return new Promise((resolve, reject) => {
      const process = spawn('python', [scriptPath]);

      let stdout = '';
      let stderr = '';

      process.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      process.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      // Send options via stdin
      process.stdin.write(JSON.stringify(options));
      process.stdin.end();

      process.on('close', (code) => {
        if (code === 0) {
          try {
            const result = JSON.parse(stdout);
            resolve(result);
          } catch (e) {
            resolve({ output: stdout, raw: true });
          }
        } else {
          reject(new Error(`ML ìŠ¤í¬ë¦½íŠ¸ ì‹¤íŒ¨ (exit code: ${code})\\n${stderr}`));
        }
      });

      process.on('error', (error) => {
        reject(new Error(`ML í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: ${error.message}`));
      });

      // Set timeout (5 minutes for complex ML operations)
      setTimeout(() => {
        process.kill('SIGKILL');
        reject(new Error('ML ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ (5ë¶„)'));
      }, 300000);
    });
  }

  /**
   * Get service status with ML-specific metrics
   */
  getStatus() {
    const baseStatus = super.getStatus();
    return {
      ...baseStatus,
      toolCount: 8,
      focus: 'machine_learning',
      features: ['classification', 'regression', 'clustering', 'time_series', 'hyperparameter_tuning', 'feature_engineering'],
      modelCache: {
        size: this.modelCache.size,
        keys: Array.from(this.modelCache.keys())
      }
    };
  }

  /**
   * Clear model cache
   */
  clearModelCache() {
    this.modelCache.clear();
    this.logger.info('ëª¨ë¸ ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤');
  }
}

export default MachineLearningService;