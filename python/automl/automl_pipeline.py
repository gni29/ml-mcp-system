#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AutoML Pipeline
ìë™ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸

ì´ ëª¨ë“ˆì€ ìë™í™”ëœ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥:
- ìë™ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹
- íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìë™í™”
- ëª¨ë¸ ì•™ìƒë¸”
- ì„±ëŠ¥ ìµœì í™”
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™í™”
"""

import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Union
import warnings
warnings.filterwarnings('ignore')

# ê³µìœ  ìœ í‹¸ë¦¬í‹° ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent / "ml-mcp-shared" / "python"))

try:
    from common_utils import load_data, get_data_info, create_analysis_result, output_results, validate_required_params
except ImportError:
    # ê³µìœ  ìœ í‹¸ë¦¬í‹° import ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ êµ¬í˜„
    def load_data(file_path: str) -> pd.DataFrame:
        """ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        file_path = Path(file_path)
        if file_path.suffix.lower() == '.csv':
            return pd.read_csv(file_path)
        elif file_path.suffix.lower() in ['.xlsx', '.xls']:
            return pd.read_excel(file_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}")

    def get_data_info(df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„°í”„ë ˆì„ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        return {
            "shape": df.shape,
            "columns": df.columns.tolist(),
            "numeric_columns": df.select_dtypes(include=[np.number]).columns.tolist(),
            "categorical_columns": df.select_dtypes(include=['object', 'category']).columns.tolist()
        }

    def create_analysis_result(analysis_type: str, data_info: Dict[str, Any], results: Dict[str, Any], summary: str = None) -> Dict[str, Any]:
        """í‘œì¤€í™”ëœ ë¶„ì„ ê²°ê³¼ êµ¬ì¡° ìƒì„±"""
        return {
            "analysis_type": analysis_type,
            "timestamp": pd.Timestamp.now().isoformat(),
            "data_info": data_info,
            "summary": summary or f"{analysis_type} ë¶„ì„ ì™„ë£Œ",
            **results
        }

    def output_results(results: Dict[str, Any]):
        """ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì¶œë ¥"""
        def comprehensive_json_serializer(obj):
            """í¬ê´„ì ì¸ JSON ì§ë ¬í™” í•¨ìˆ˜"""
            if isinstance(obj, (np.int8, np.int16, np.int32, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.float16, np.float32, np.float64)):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, pd.Index):
                return obj.tolist()
            elif isinstance(obj, pd.Series):
                return obj.tolist()
            elif isinstance(obj, type) or str(type(obj)).startswith("<class 'numpy."):
                return str(obj)
            elif hasattr(obj, 'item'):
                return obj.item()
            elif hasattr(obj, 'dtype') and hasattr(obj, 'name'):
                return str(obj)
            elif hasattr(obj, 'to_dict') and callable(obj.to_dict):
                return obj.to_dict()
            elif hasattr(obj, '__dict__') and not isinstance(obj, type):
                return str(obj)
            else:
                return str(obj)

        def clean_dict_for_json(data):
            """ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ì™€ ê°’ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ì •ë¦¬"""
            if isinstance(data, dict):
                cleaned = {}
                for key, value in data.items():
                    clean_key = str(key) if not isinstance(key, (str, int, float, bool, type(None))) else key
                    cleaned[clean_key] = clean_dict_for_json(value)
                return cleaned
            elif isinstance(data, (list, tuple)):
                return [clean_dict_for_json(item) for item in data]
            else:
                return comprehensive_json_serializer(data)

        try:
            cleaned_results = clean_dict_for_json(results)
            print(json.dumps(cleaned_results, ensure_ascii=False, indent=2, default=comprehensive_json_serializer))
        except Exception as e:
            error_output = {
                "success": False,
                "error": f"JSON ì§ë ¬í™” ì‹¤íŒ¨: {str(e)}",
                "error_type": "SerializationError"
            }
            print(json.dumps(error_output, ensure_ascii=False, indent=2))

    def validate_required_params(params: Dict[str, Any], required: list):
        """í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ ê²€ì¦"""
        missing = [param for param in required if param not in params]
        if missing:
            raise ValueError(f"í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ê°€ ëˆ„ë½ë¨: {', '.join(missing)}")

try:
    from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
    from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel
    from sklearn.ensemble import VotingClassifier, VotingRegressor
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
    from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
    from sklearn.svm import SVC, SVR
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
    from sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error
    from sklearn.pipeline import Pipeline
    import joblib
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

class AutoMLPipeline:
    """ìë™ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        self.best_model = None
        self.preprocessing_pipeline = None
        self.feature_selector = None
        self.search_results = {}
        self.ensemble_model = None

    def run_automl(self, df: pd.DataFrame, target_column: str, task_type: str = "auto",
                   time_budget: int = 300, optimization_metric: str = "auto",
                   include_ensembles: bool = True, max_models: int = 10) -> Dict[str, Any]:
        """
        AutoML íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Parameters:
        -----------
        df : pd.DataFrame
            í›ˆë ¨ìš© ë°ì´í„°í”„ë ˆì„
        target_column : str
            íƒ€ê²Ÿ ë³€ìˆ˜ ì»¬ëŸ¼ëª…
        task_type : str, default="auto"
            ì‘ì—… ìœ í˜• ("classification", "regression", "auto")
        time_budget : int, default=300
            ìµœì í™” ì‹œê°„ ì˜ˆì‚° (ì´ˆ)
        optimization_metric : str, default="auto"
            ìµœì í™” ë©”íŠ¸ë¦­
        include_ensembles : bool, default=True
            ì•™ìƒë¸” ëª¨ë¸ í¬í•¨ ì—¬ë¶€
        max_models : int, default=10
            ìµœëŒ€ ì‹œë„í•  ëª¨ë¸ ìˆ˜

        Returns:
        --------
        Dict[str, Any]
            AutoML ê²°ê³¼
        """

        if not SKLEARN_AVAILABLE:
            return {
                "error": "scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                "required_package": "scikit-learn"
            }

        try:
            results = {
                "success": True,
                "automl_config": {
                    "task_type": task_type,
                    "time_budget": time_budget,
                    "optimization_metric": optimization_metric,
                    "include_ensembles": include_ensembles,
                    "max_models": max_models
                },
                "pipeline_steps": [],
                "model_results": {},
                "best_model": {},
                "performance_comparison": {},
                "feature_engineering": {}
            }

            # 1. ë°ì´í„° ë¶„ì„ ë° ì‘ì—… ìœ í˜• ê²°ì •
            print("ğŸ” ë°ì´í„° ë¶„ì„ ì¤‘...", file=sys.stderr)
            task_type = self.determine_task_type(df, target_column) if task_type == "auto" else task_type
            results["automl_config"]["determined_task_type"] = task_type
            results["pipeline_steps"].append("ë°ì´í„° ë¶„ì„ ë° ì‘ì—… ìœ í˜• ê²°ì •")

            # 2. ë°ì´í„° ì „ì²˜ë¦¬
            print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...", file=sys.stderr)
            X_processed, y_processed, preprocessing_info = self.preprocess_data(df, target_column, task_type)
            results["preprocessing_info"] = preprocessing_info
            results["pipeline_steps"].append("ë°ì´í„° ì „ì²˜ë¦¬")

            # 3. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
            print("âš™ï¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì¤‘...", file=sys.stderr)
            X_engineered, feature_engineering_info = self.perform_feature_engineering(
                X_processed, y_processed, task_type
            )
            results["feature_engineering"] = feature_engineering_info
            results["pipeline_steps"].append("íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")

            # 4. í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X_engineered, y_processed, test_size=0.2, random_state=42,
                stratify=y_processed if task_type == "classification" else None
            )

            # 5. ëª¨ë¸ ì„ íƒ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            print("ğŸ¤– ëª¨ë¸ ìµœì í™” ì¤‘...", file=sys.stderr)
            model_results = self.optimize_models(
                X_train, X_test, y_train, y_test, task_type,
                time_budget, optimization_metric, max_models
            )
            results["model_results"] = model_results
            results["pipeline_steps"].append("ëª¨ë¸ ìµœì í™”")

            # 6. ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
            if include_ensembles and len(model_results) >= 2:
                print("ğŸ­ ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...", file=sys.stderr)
                ensemble_result = self.create_ensemble(
                    model_results, X_train, X_test, y_train, y_test, task_type
                )
                results["ensemble_result"] = ensemble_result
                results["pipeline_steps"].append("ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")

            # 7. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
            best_model_info = self.select_best_model(results.get("model_results", {}),
                                                   results.get("ensemble_result", {}), task_type)
            results["best_model"] = best_model_info
            results["pipeline_steps"].append("ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ")

            # 8. ëª¨ë¸ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸
            print("ğŸ“Š ëª¨ë¸ í•´ì„ ì¤‘...", file=sys.stderr)
            interpretation_results = self.interpret_model(
                best_model_info, X_engineered.columns.tolist() if hasattr(X_engineered, 'columns') else None
            )
            results["model_interpretation"] = interpretation_results
            results["pipeline_steps"].append("ëª¨ë¸ í•´ì„")

            # 9. ê¶Œê³ ì‚¬í•­ ìƒì„±
            recommendations = self.generate_automl_recommendations(results)
            results["recommendations"] = recommendations

            return results

        except Exception as e:
            return {
                "success": False,
                "error": f"AutoML íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}",
                "error_type": type(e).__name__
            }

    def determine_task_type(self, df: pd.DataFrame, target_column: str) -> str:
        """ì‘ì—… ìœ í˜• ìë™ ê²°ì •"""

        target_series = df[target_column]
        unique_values = target_series.nunique()
        total_values = len(target_series)

        # ìˆ˜ì¹˜í˜•ì´ì§€ë§Œ ì¹´í…Œê³ ë¦¬ê°€ ì ì€ ê²½ìš°
        if pd.api.types.is_numeric_dtype(target_series):
            if unique_values <= 10 and unique_values < total_values * 0.1:
                return "classification"
            else:
                return "regression"
        else:
            return "classification"

    def preprocess_data(self, df: pd.DataFrame, target_column: str, task_type: str) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
        """ë°ì´í„° ì „ì²˜ë¦¬"""

        preprocessing_info = {
            "steps_performed": [],
            "missing_values": {},
            "categorical_encoding": {},
            "scaling": {},
            "feature_count": {"original": 0, "final": 0}
        }

        # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
        y = df[target_column].copy()
        X = df.drop(columns=[target_column]).copy()
        preprocessing_info["feature_count"]["original"] = X.shape[1]

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        missing_info = self.handle_missing_values(X)
        preprocessing_info["missing_values"] = missing_info
        if missing_info["values_imputed"] > 0:
            preprocessing_info["steps_performed"].append("ê²°ì¸¡ê°’ ì²˜ë¦¬")

        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        encoding_info = self.encode_categorical_variables(X)
        preprocessing_info["categorical_encoding"] = encoding_info
        if encoding_info["columns_encoded"] > 0:
            preprocessing_info["steps_performed"].append("ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©")

        # íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬
        if task_type == "classification" and y.dtype == 'object':
            le = LabelEncoder()
            y_processed = le.fit_transform(y)
            preprocessing_info["target_encoding"] = {
                "classes": le.classes_.tolist(),
                "encoded": True
            }
        else:
            y_processed = y.values

        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        preprocessing_info["scaling"] = {
            "method": "StandardScaler",
            "applied": True
        }
        preprocessing_info["steps_performed"].append("íŠ¹ì„± ìŠ¤ì¼€ì¼ë§")

        preprocessing_info["feature_count"]["final"] = X_scaled.shape[1]

        return X_scaled, y_processed, preprocessing_info

    def handle_missing_values(self, X: pd.DataFrame) -> Dict[str, Any]:
        """ê²°ì¸¡ê°’ ì²˜ë¦¬"""

        missing_info = {
            "original_missing": X.isnull().sum().sum(),
            "values_imputed": 0,
            "strategies": {}
        }

        if missing_info["original_missing"] > 0:
            for col in X.columns:
                if X[col].isnull().sum() > 0:
                    if pd.api.types.is_numeric_dtype(X[col]):
                        # ìˆ˜ì¹˜í˜•: ì¤‘ìœ„ê°’
                        median_val = X[col].median()
                        X[col].fillna(median_val, inplace=True)
                        missing_info["strategies"][col] = f"ì¤‘ìœ„ê°’ ëŒ€ì²´ ({median_val})"
                    else:
                        # ë²”ì£¼í˜•: ìµœë¹ˆê°’
                        mode_val = X[col].mode()[0] if len(X[col].mode()) > 0 else 'unknown'
                        X[col].fillna(mode_val, inplace=True)
                        missing_info["strategies"][col] = f"ìµœë¹ˆê°’ ëŒ€ì²´ ({mode_val})"

            missing_info["values_imputed"] = missing_info["original_missing"]

        return missing_info

    def encode_categorical_variables(self, X: pd.DataFrame) -> Dict[str, Any]:
        """ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©"""

        encoding_info = {
            "columns_encoded": 0,
            "encoding_methods": {},
            "new_features_created": 0
        }

        categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()

        for col in categorical_columns:
            unique_count = X[col].nunique()

            if unique_count <= 5:
                # One-hot ì¸ì½”ë”©
                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)
                X = X.drop(columns=[col])
                X = pd.concat([X, dummies], axis=1)
                encoding_info["encoding_methods"][col] = f"One-hot ì¸ì½”ë”© ({unique_count-1}ê°œ íŠ¹ì„± ìƒì„±)"
                encoding_info["new_features_created"] += unique_count - 1
            else:
                # Label ì¸ì½”ë”©
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
                encoding_info["encoding_methods"][col] = "Label ì¸ì½”ë”©"

            encoding_info["columns_encoded"] += 1

        return encoding_info

    def perform_feature_engineering(self, X: np.ndarray, y: np.ndarray, task_type: str) -> Tuple[np.ndarray, Dict[str, Any]]:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""

        feature_eng_info = {
            "techniques_applied": [],
            "original_features": X.shape[1],
            "final_features": 0,
            "feature_selection": {}
        }

        X_engineered = X.copy()

        # íŠ¹ì„± ì„ íƒ
        if X.shape[1] > 20:  # íŠ¹ì„±ì´ ë§ì€ ê²½ìš°ì—ë§Œ ì ìš©
            selector = SelectKBest(k=min(15, X.shape[1]))
            X_engineered = selector.fit_transform(X_engineered, y)

            selected_features = selector.get_support(indices=True)
            feature_eng_info["feature_selection"] = {
                "method": "SelectKBest",
                "features_selected": len(selected_features),
                "selected_indices": selected_features.tolist()
            }
            feature_eng_info["techniques_applied"].append("íŠ¹ì„± ì„ íƒ")

        feature_eng_info["final_features"] = X_engineered.shape[1]

        return X_engineered, feature_eng_info

    def optimize_models(self, X_train, X_test, y_train, y_test, task_type: str,
                       time_budget: int, optimization_metric: str, max_models: int) -> Dict[str, Any]:
        """ëª¨ë¸ ìµœì í™”"""

        # ëª¨ë¸ í›„ë³´êµ° ì •ì˜
        if task_type == "classification":
            models = {
                "random_forest": RandomForestClassifier(random_state=42),
                "logistic_regression": LogisticRegression(random_state=42, max_iter=1000),
                "gradient_boosting": GradientBoostingClassifier(random_state=42),
                "svm": SVC(random_state=42, probability=True),
                "knn": KNeighborsClassifier()
            }
            scoring_metric = "accuracy" if optimization_metric == "auto" else optimization_metric
        else:
            models = {
                "random_forest": RandomForestRegressor(random_state=42),
                "linear_regression": LinearRegression(),
                "gradient_boosting": GradientBoostingRegressor(random_state=42),
                "ridge": Ridge(random_state=42),
                "knn": KNeighborsRegressor()
            }
            scoring_metric = "r2" if optimization_metric == "auto" else optimization_metric

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        param_grids = self.get_hyperparameter_grids()

        model_results = {}
        time_per_model = time_budget // min(max_models, len(models))

        for model_name, model in list(models.items())[:max_models]:
            print(f"  - {model_name} ìµœì í™” ì¤‘...", file=sys.stderr)

            try:
                # ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰
                param_grid = param_grids.get(model_name, {})

                if param_grid:
                    # RandomizedSearchë¡œ ë¹ ë¥¸ ìµœì í™”
                    search = RandomizedSearchCV(
                        model, param_grid, n_iter=min(20, len(param_grid)),
                        cv=3, scoring=scoring_metric, random_state=42,
                        n_jobs=-1
                    )
                    search.fit(X_train, y_train)
                    best_model = search.best_estimator_
                else:
                    # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
                    best_model = model
                    best_model.fit(X_train, y_train)

                # ì„±ëŠ¥ í‰ê°€
                y_pred = best_model.predict(X_test)

                if task_type == "classification":
                    score = accuracy_score(y_test, y_pred)
                    f1 = f1_score(y_test, y_pred, average='weighted')
                    metrics = {"accuracy": score, "f1_score": f1}
                else:
                    score = r2_score(y_test, y_pred)
                    mse = mean_squared_error(y_test, y_pred)
                    metrics = {"r2_score": score, "mse": mse}

                model_results[model_name] = {
                    "model": best_model,
                    "score": float(score),
                    "metrics": {k: float(v) for k, v in metrics.items()},
                    "hyperparameters": best_model.get_params() if hasattr(best_model, 'get_params') else {}
                }

            except Exception as e:
                print(f"    {model_name} ìµœì í™” ì‹¤íŒ¨: {str(e)}", file=sys.stderr)
                continue

        return model_results

    def get_hyperparameter_grids(self) -> Dict[str, Dict]:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜"""

        return {
            "random_forest": {
                "n_estimators": [50, 100, 200],
                "max_depth": [None, 10, 20],
                "min_samples_split": [2, 5, 10]
            },
            "logistic_regression": {
                "C": [0.1, 1, 10],
                "penalty": ["l1", "l2"],
                "solver": ["liblinear", "lbfgs"]
            },
            "gradient_boosting": {
                "n_estimators": [50, 100, 200],
                "learning_rate": [0.01, 0.1, 0.2],
                "max_depth": [3, 5, 7]
            },
            "svm": {
                "C": [0.1, 1, 10],
                "kernel": ["rbf", "linear"],
                "gamma": ["scale", "auto"]
            },
            "ridge": {
                "alpha": [0.1, 1, 10, 100]
            },
            "knn": {
                "n_neighbors": [3, 5, 7, 9],
                "weights": ["uniform", "distance"]
            }
        }

    def create_ensemble(self, model_results: Dict, X_train, X_test, y_train, y_test, task_type: str) -> Dict[str, Any]:
        """ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""

        if len(model_results) < 2:
            return {"error": "ì•™ìƒë¸”ì„ ìœ„í•œ ì¶©ë¶„í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤"}

        try:
            # ìƒìœ„ ì„±ëŠ¥ ëª¨ë¸ë“¤ ì„ íƒ
            sorted_models = sorted(model_results.items(), key=lambda x: x[1]["score"], reverse=True)
            top_models = sorted_models[:min(3, len(sorted_models))]

            models_for_ensemble = [(name, info["model"]) for name, info in top_models]

            if task_type == "classification":
                ensemble = VotingClassifier(models_for_ensemble, voting='soft')
            else:
                ensemble = VotingRegressor(models_for_ensemble)

            ensemble.fit(X_train, y_train)
            y_pred = ensemble.predict(X_test)

            if task_type == "classification":
                score = accuracy_score(y_test, y_pred)
                f1 = f1_score(y_test, y_pred, average='weighted')
                metrics = {"accuracy": score, "f1_score": f1}
            else:
                score = r2_score(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                metrics = {"r2_score": score, "mse": mse}

            return {
                "ensemble_model": ensemble,
                "component_models": [name for name, _ in models_for_ensemble],
                "score": float(score),
                "metrics": {k: float(v) for k, v in metrics.items()},
                "improvement_over_best_single": float(score - max([info["score"] for info in model_results.values()]))
            }

        except Exception as e:
            return {"error": f"ì•™ìƒë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}"}

    def select_best_model(self, model_results: Dict, ensemble_result: Dict, task_type: str) -> Dict[str, Any]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ"""

        all_results = model_results.copy()
        if "ensemble_model" in ensemble_result:
            all_results["ensemble"] = {
                "score": ensemble_result["score"],
                "metrics": ensemble_result["metrics"],
                "model": ensemble_result["ensemble_model"]
            }

        if not all_results:
            return {"error": "ì„ íƒí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤"}

        best_model_name = max(all_results.keys(), key=lambda x: all_results[x]["score"])
        best_model_info = all_results[best_model_name]

        return {
            "best_model_name": best_model_name,
            "best_score": best_model_info["score"],
            "best_metrics": best_model_info["metrics"],
            "model_object": best_model_info["model"],
            "is_ensemble": best_model_name == "ensemble"
        }

    def interpret_model(self, best_model_info: Dict, feature_names: Optional[List[str]]) -> Dict[str, Any]:
        """ëª¨ë¸ í•´ì„"""

        interpretation = {
            "model_type": type(best_model_info["model_object"]).__name__,
            "interpretability_level": "unknown",
            "feature_importance": None,
            "model_complexity": "unknown"
        }

        try:
            model = best_model_info["model_object"]

            # íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                interpretation["feature_importance"] = {
                    "values": importances.tolist(),
                    "feature_names": feature_names[:len(importances)] if feature_names else None
                }
                interpretation["interpretability_level"] = "high"

            elif hasattr(model, 'coef_'):
                coef = model.coef_
                if coef.ndim > 1:
                    coef = coef[0]
                interpretation["feature_importance"] = {
                    "values": coef.tolist(),
                    "feature_names": feature_names[:len(coef)] if feature_names else None
                }
                interpretation["interpretability_level"] = "high"

            else:
                interpretation["interpretability_level"] = "low"

            # ëª¨ë¸ ë³µì¡ë„
            if hasattr(model, 'n_estimators'):
                interpretation["model_complexity"] = f"ì•™ìƒë¸” ({model.n_estimators} ì¶”ì •ê¸°)"
            elif hasattr(model, 'support_vectors_'):
                interpretation["model_complexity"] = f"SVM ({len(model.support_vectors_)} ì„œí¬íŠ¸ ë²¡í„°)"
            else:
                interpretation["model_complexity"] = "ë‹¨ìˆœ ëª¨ë¸"

        except Exception as e:
            interpretation["error"] = str(e)

        return interpretation

    def generate_automl_recommendations(self, results: Dict[str, Any]) -> List[Dict[str, str]]:
        """AutoML ê¶Œê³ ì‚¬í•­ ìƒì„±"""

        recommendations = []

        # ìµœê³  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        if "best_model" in results and "best_score" in results["best_model"]:
            best_score = results["best_model"]["best_score"]

            if best_score < 0.7:
                recommendations.append({
                    "type": "performance",
                    "priority": "high",
                    "issue": f"ëª¨ë¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤ ({best_score:.3f})",
                    "recommendation": "ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë˜ëŠ” ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”",
                    "action": "íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê°•í™”, ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„ "
                })

        # íŠ¹ì„± ìˆ˜ ëŒ€ë¹„ ë°ì´í„° ìˆ˜
        if "preprocessing_info" in results and "feature_count" in results["preprocessing_info"]:
            feature_count = results["preprocessing_info"]["feature_count"]["final"]

            recommendations.append({
                "type": "feature_optimization",
                "priority": "medium",
                "issue": f"íŠ¹ì„± ìˆ˜: {feature_count}ê°œ",
                "recommendation": "íŠ¹ì„± ì„ íƒ ë˜ëŠ” ì°¨ì› ì¶•ì†Œ ê³ ë ¤",
                "action": "PCA, íŠ¹ì„± ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ"
            })

        # ì•™ìƒë¸” íš¨ê³¼
        if "ensemble_result" in results and "improvement_over_best_single" in results["ensemble_result"]:
            improvement = results["ensemble_result"]["improvement_over_best_single"]

            if improvement > 0.01:
                recommendations.append({
                    "type": "ensemble_success",
                    "priority": "info",
                    "issue": f"ì•™ìƒë¸”ë¡œ {improvement:.3f} ì„±ëŠ¥ í–¥ìƒ",
                    "recommendation": "ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš© ê¶Œì¥",
                    "action": "í”„ë¡œë•ì…˜ì—ì„œ ì•™ìƒë¸” ëª¨ë¸ ë°°í¬"
                })

        return recommendations

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # stdinì—ì„œ JSON ë°ì´í„° ì½ê¸°
        input_data = sys.stdin.read()
        params = json.loads(input_data)

        # í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ ê²€ì¦
        validate_required_params(params, ['target_column'])

        # íŒŒì¼ ê²½ë¡œê°€ ì œê³µëœ ê²½ìš° íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        if 'file_path' in params:
            df = load_data(params['file_path'])
        else:
            # JSON ë°ì´í„°ì—ì„œ ì§ì ‘ DataFrame ìƒì„±
            if 'data' in params:
                df = pd.DataFrame(params['data'])
            else:
                df = pd.DataFrame(params)

        # AutoML ì˜µì…˜
        target_column = params['target_column']
        task_type = params.get('task_type', 'auto')
        time_budget = params.get('time_budget', 300)
        optimization_metric = params.get('optimization_metric', 'auto')
        include_ensembles = params.get('include_ensembles', True)
        max_models = params.get('max_models', 10)

        # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        data_info = get_data_info(df)

        # AutoML ì‹¤í–‰
        automl_pipeline = AutoMLPipeline()
        automl_result = automl_pipeline.run_automl(
            df, target_column, task_type, time_budget,
            optimization_metric, include_ensembles, max_models
        )

        if not automl_result.get('success', False):
            error_result = {
                "success": False,
                "error": automl_result.get('error', 'AutoML íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨'),
                "analysis_type": "automl_pipeline"
            }
            output_results(error_result)
            return

        # ë¶„ì„ ê²°ê³¼ í†µí•©
        analysis_results = {
            "automl_pipeline": automl_result,
            "automl_summary": {
                "pipeline_steps": len(automl_result.get('pipeline_steps', [])),
                "models_trained": len(automl_result.get('model_results', {})),
                "best_model": automl_result.get('best_model', {}).get('best_model_name', 'None'),
                "best_score": automl_result.get('best_model', {}).get('best_score', 0.0),
                "recommendations_count": len(automl_result.get('recommendations', []))
            }
        }

        # ìš”ì•½ ìƒì„±
        steps_count = len(automl_result.get('pipeline_steps', []))
        models_count = len(automl_result.get('model_results', {}))
        best_model = automl_result.get('best_model', {}).get('best_model_name', 'None')
        best_score = automl_result.get('best_model', {}).get('best_score', 0.0)
        summary = f"AutoML íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - {steps_count}ë‹¨ê³„ ìˆ˜í–‰, {models_count}ê°œ ëª¨ë¸ í›ˆë ¨, ìµœê³  ì„±ëŠ¥: {best_model} ({best_score:.3f})"

        # í‘œì¤€í™”ëœ ê²°ê³¼ ìƒì„±
        final_result = create_analysis_result(
            analysis_type="automl_pipeline",
            data_info=data_info,
            results=analysis_results,
            summary=summary
        )

        # ê²°ê³¼ ì¶œë ¥
        output_results(final_result)

    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "analysis_type": "automl_pipeline",
            "timestamp": pd.Timestamp.now().isoformat()
        }
        output_results(error_result)
        sys.exit(1)

if __name__ == "__main__":
    main()